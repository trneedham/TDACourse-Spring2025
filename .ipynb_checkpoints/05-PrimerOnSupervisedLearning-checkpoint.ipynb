{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e187b071-dc16-4a37-92ea-b92d17275ed2",
   "metadata": {},
   "source": [
    "# Primer on Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094447a2-181e-4914-abb0-0da875138980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147110e3-238b-4581-be3b-2635725488ca",
   "metadata": {},
   "source": [
    "# General Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9bdf7-66b3-4b78-8443-408b5b09d0d2",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "Let $X = \\{x_1,\\ldots,x_N\\}$ be a collection of vectors in $\\mathbb{R}^d$, and assume that each vector $x_i$ comes with a *label* $y_i \\in \\{0,1\\}$. Let $y = \\{y_1,\\ldots,y_N\\}$ denote the set of labels.\n",
    "\n",
    "**Example:** Each $x_i$ could be a greyscale image of size $m \\times n$ pixels. Then we can consider $x_i$ as a vector in $\\mathbb{R}^m \\times \\mathbb{R}^n \\approx \\mathbb{R}^{mn}$ (so $d = mn$). Suppose that the dataset consists of pictures of *dogs* (label 0) and *cats* (label 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7ece1-c699-4b89-8556-6822c7f46b38",
   "metadata": {},
   "source": [
    "**Goal:** Train a classifier to automatically determine if an element of $\\mathbb{R}^d$ belongs to class 0 or class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eac981c-aa06-42e2-8370-f92dd06e7e96",
   "metadata": {},
   "source": [
    "## Supervised Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfff98f-995f-424a-aba2-b310fb435d79",
   "metadata": {},
   "source": [
    "We fix a finite-dimensional class of functions $\\mathcal{F}$ of the form $f:\\mathbb{R}^d \\to [0,1]$. Usually, $\\mathcal{F}$ is *parameterized* by $\\mathbb{R}^n$; that is, for a selection of parameters $\\theta \\in \\mathbb{R}^n$, there is a function $f_\\theta \\in \\mathcal{F}$ which is determined by these parameters.\n",
    "\n",
    "We think of each function $f_\\theta$ as a *classifier*: given $x \\in \\mathbb{R}^d$, \n",
    "- if $f_\\theta(x) \\leq 0.5$, then $x$ is classified as class 0,\n",
    "- if $f_\\theta(x) > 0.5$, then $x$ is classified as class 1.\n",
    "\n",
    "Our task is to find the *best* choice of parameters; that is, the choice $\\theta \\in \\mathbb{R}^n$ giving a classifier $f_\\theta$ with the best performance. The question is: **how do we measure performance of a classifier**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a87dbb-dba9-450b-a6f1-a1943a987ccd",
   "metadata": {},
   "source": [
    "### Quantifying Classifer Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed2ff1-4532-4bcf-8a2f-556aaeb7381d",
   "metadata": {},
   "source": [
    "We typically choose a subset (the **training set**) of our data $X_{train} \\subset X$, $y_{train} \\subset y$. \n",
    "\n",
    "Next we choose a **loss function** $L:\\mathbb{R}^n \\to \\mathbb{R}_{\\geq 0}$ from the parameter space to $\\mathbb{R}$. The loss function is typically of the form\n",
    "$$\n",
    "L(\\theta) = \\sum_{(x_i,y_i) \\in X_{train} \\times y_{train}} \\ell(\\theta,x_i,y_i),\n",
    "$$\n",
    "and should qualitatively have the properties:\n",
    "- if $\\theta$ leads to a classifier $f_\\theta$ which typically gives the **correct** classification on the training set, then $L(\\theta)$ should be **small**\n",
    "- if $\\theta$ leads to a classifier $f_\\theta$ which typically gives the **incorrect** classification on the training set, then $L(\\theta)$ should be **large**\n",
    "\n",
    "Then we can refine our **Goal:** minimize $L(\\theta)$ over $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c2d5c-1c7e-4373-858f-3b6ba4d12885",
   "metadata": {},
   "source": [
    "## Example: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394db7b-acbd-4b6e-a26c-630d65ea0fcc",
   "metadata": {},
   "source": [
    "Consider **parameters** of the form $\\theta = (\\theta_1,\\ldots,\\theta_d,\\theta_{d+1}) \\in \\mathbb{R}^{d+1}$ (so $n = d+1$ in this example).\n",
    "\n",
    "The **classification function** corresponding to $\\theta$ is \n",
    "$$\n",
    "f_\\theta(x) = \\frac{1}{1 + \\exp(-(\\langle (\\theta_1,\\ldots,\\theta_d), x \\rangle + \\theta_{d+1}))}\n",
    "$$\n",
    "Geometrically, $f_\\theta(x)$ is:\n",
    "- close to zero (i.e., predicts class 0) when $\\langle (\\theta_1,\\ldots,\\theta_d), x \\rangle + \\theta_{d+1} < 0$\n",
    "- close to one (i.e., predicts class 1) when $\\langle (\\theta_1,\\ldots,\\theta_d), x \\rangle + \\theta_{d+1} > 0$.\n",
    "\n",
    "The two cases correspond to which \"side\" $x$ falls on, with respect to the affine hyperplane $\\theta_1 x_1 + \\cdots + \\theta_d x_d + \\theta_{d+1} = 0$. \n",
    "\n",
    "The **loss function** is \n",
    "$$\n",
    "L(\\theta) = - \\sum_{(x_i,y_i)} \\big(y_i \\log (f_\\theta(x_i)) + (1-y_i) \\log(1-f_\\theta(x_i)) \\big)\n",
    "$$\n",
    "The summand \n",
    "$$\n",
    "\\ell(\\theta,x,y) = -\\big(y \\log(f_\\theta(x)) + (1-y) \\log(1-f_\\theta(x))\\big)\n",
    "$$\n",
    "has the properties that:\n",
    "- if $y=0$ and $f_\\theta(x)$ is close to zero, $\\ell(\\theta,x,y)$ is small\n",
    "- if $y=0$ and $f_\\theta(x)$ is close to one, $\\ell(\\theta,x,y)$ is big\n",
    "- if $y=1$ and $f_\\theta(x)$ is close to zero, $\\ell(\\theta,x,y)$ is big\n",
    "- if $y=1$ and $f_\\theta(x)$ is close to one, $\\ell(\\theta,x,y)$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcc457-fc49-4815-a685-e9121bbe9867",
   "metadata": {},
   "source": [
    "## Logistic Regression in Practice\n",
    "\n",
    "Let's run logistic regression on a real dataset.\n",
    "\n",
    "We'll use a shrunken down version of the classic MNIST dataset, consisting of many samples of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3df534-789a-4492-93fa-27c1e19da8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3a8ad-fe8a-43e3-87d3-15c84bf7c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.imshow(digits.images[j], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3f373-262c-4abb-bd1c-2f6d37663028",
   "metadata": {},
   "source": [
    "Let's construct a dataset consisting of only the 0's and 1's.\n",
    "\n",
    "(Logistic regression (and other supervised classification problems) can handle more than two classes, but let's keep it simple to illustrate the idea here.)\n",
    "\n",
    "We'll also split off part of the data as a training set. We keep the rest as a **test set**, to test our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30e4fc-84c5-4225-b5bc-f274718495a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MNIST = digits.data\n",
    "MNISTlabels = digits.target\n",
    "\n",
    "X = np.vstack((MNIST[MNISTlabels==0],MNIST[MNISTlabels==1]))\n",
    "y = np.hstack((MNISTlabels[MNISTlabels==0],MNISTlabels[MNISTlabels==1]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636eae9d-d805-4623-889a-684c7c78494a",
   "metadata": {},
   "source": [
    "Optimizing the parameters $\\theta$ in logistic regression is handled by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533563b-9314-4346-afa1-d5b73ca885f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e1d37-c90a-4fc6-8e4a-a48d750becd0",
   "metadata": {},
   "source": [
    "We can see the optimal parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec21047-3d96-483f-ae86-705ffae95064",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edf2f9-c23c-48a1-9cd6-780dc021baff",
   "metadata": {},
   "source": [
    "The coefficients above give $\\theta_1,\\ldots,\\theta_d$. We can see this by looking at the shape of the coefficients vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc32569-30e2-41c8-ba09-7430d9fb4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64d483-ad37-4d43-96cf-7e805ff9095d",
   "metadata": {},
   "source": [
    "The last parameter $\\theta_{d+1}$ is stored in the `intercept` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857708d-927f-4717-9746-2102b2063f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec75ea-0a7e-4744-beb8-ddfc5557df6c",
   "metadata": {},
   "source": [
    "Now we can use the trained model to predict classes of the digits from our testing set. Let's take a look at how it performs, qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241edb4-2df4-4aba-a4dc-afed832dda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834105a-6785-47f4-9fc3-7c256bc352d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for j in range(20):\n",
    "    fig.add_subplot(4,5,j+1)\n",
    "    plt.imshow(X_test[j].reshape(8,8), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Pred. Label='+str(X_predicted[j]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39c888-59e8-4091-bb92-bac2a80442e8",
   "metadata": {},
   "source": [
    "Quantitatively, the model performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717f10d-9b41-492a-ac4e-c545b434ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Prediction Accuracy:',np.round(model.score(X_test,y_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35116512-8561-4560-bd36-982686a805d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Prediction Accuracy:',np.round(model.score(X_train,y_train),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb14e1f-df45-478c-909a-86a0e43e938e",
   "metadata": {},
   "source": [
    "## Low-Dimensional Version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c864fc2-5c30-4da9-8c97-d3676e1b55f0",
   "metadata": {},
   "source": [
    "Let's project everything down to 2 dimensions using PCA so that we can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390587b-503f-440d-bfc0-fdbc1f5fce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b996a5-1af3-45dd-b2c3-492b0e115fe0",
   "metadata": {},
   "source": [
    "We'll find the best 2-dimensional representation of $X$ (which naturally lives in 64 dimensions) via a PCA projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2edd4-dc73-44d8-94f9-9f792bca7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_proj = pca.fit_transform(X) # Solves the PCA problem for our dataset X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e47c80-f196-4355-adaf-f25f6932880d",
   "metadata": {},
   "source": [
    "Here's a picture of the projected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca652fb-b0f1-4926-9b52-befddce94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_zeros = np.sum(y==0)\n",
    "\n",
    "plt.scatter(X_proj[:num_zeros,0],X_proj[:num_zeros,1],label='0')\n",
    "plt.scatter(X_proj[num_zeros:,0],X_proj[num_zeros:,1],label = '1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ccbce-53bc-4b59-bfce-883417dcb8f8",
   "metadata": {},
   "source": [
    "We can run logistic regression on the 2d dataset. Let's first do a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29feb6ba-0a53-426b-a0a0-7fe2c50098c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj_train, X_proj_test, y_train, y_test = train_test_split(X_proj,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0827cf3c-205c-414a-bc5d-8d33b9391e98",
   "metadata": {},
   "source": [
    "We now re-fit the model to the 2d data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431a106-eebb-4940-a309-2e03c6a29b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_proj_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f48b6-13be-4477-8af1-09ef4f9ead4a",
   "metadata": {},
   "source": [
    "The coefficients are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81471e41-5832-4dce-a5e7-f1d2010db07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128068e-24ea-41b2-9b8d-04ac208ae8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18ffa2-281a-42f2-97e0-b7bbe41135d1",
   "metadata": {},
   "source": [
    "We can plot the affine hyperplane which optimally separates the data.\n",
    "\n",
    "The first plot shows the separation of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7c1e1-176b-4339-8bc2-886dc1fe23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-15,10,100)\n",
    "a = model.coef_[0][0]\n",
    "b = model.coef_[0][1]\n",
    "c = model.intercept_[0]\n",
    "ys = [c/b - a/b*x for x in xs]\n",
    "\n",
    "plt.scatter(X_proj_train[:,0],X_proj_train[:,1], c = y_train)\n",
    "plt.plot(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac6e86-e8fd-480d-96ee-c54b0e240427",
   "metadata": {},
   "source": [
    "Now let's look at the separation of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcaf1e2-d281-4927-8859-ae47ea600a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_proj_test[:,0],X_proj_test[:,1], c = y_test)\n",
    "plt.plot(xs,ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d273a-b539-4e02-8311-9ca2e529430b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf3d6c-ed90-4c4c-830d-386bdb8aee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
